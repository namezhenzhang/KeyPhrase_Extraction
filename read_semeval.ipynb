{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "datadir_train = r\"datasets\\semeval2017\\train\\train2\"\r\n",
    "datadir_train_abs = r\"D:\\mytsinghua\\nlp\\KeyPhrase_Extraction\\datasets\\semeval2017\\test\\semeval_articles_test\"\r\n",
    "datadir_test = r\"datasets\\semeval2017\\test\\semeval_articles_test\"\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fileNames(root, suffix=None):\r\n",
    "    \r\n",
    "    if not os.path.isabs(root):\r\n",
    "        root = os.path.join(os.getcwd(),root)\r\n",
    "    names = os.listdir(root)\r\n",
    "    result = []\r\n",
    "    if suffix:\r\n",
    "        for name in names:\r\n",
    "            if os.path.splitext(name)[1] == suffix:\r\n",
    "                result.append(os.path.splitext(name)[0])\r\n",
    "    else:\r\n",
    "        result = names\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alldata = fileNames(datadir_train,'.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "from transformers import RobertaTokenizerFast,RobertaTokenizer,BertTokenizer\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\",add_prefix_space=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "tokenizer2 = RobertaTokenizer.from_pretrained(\"roberta-base\",add_prefix_space=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "tokenizer3 = BertTokenizer.from_pretrained(\"bert-base-uncased\",add_prefix_space=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 917kB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 828kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 1.12MB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "source": [
    "t = \"ObjectiveElectrically evoked auditory steady-state responses (EASSRs) are neural potentials measured in the electroencephalogram (EEG) in response to periodic pulse trains presented, for example, through a cochlear implant (CI). EASSRs could potentially be used for objective CI fitting. However, EEG signals are contaminated with electrical CI artifacts. In this paper, we characterized the CI artifacts for monopolar mode stimulation and evaluated at which pulse rate, linear interpolation over the signal part contaminated with CI artifact is successful.MethodsCI artifacts were characterized by means of their amplitude growth functions and duration.ResultsCI artifact durations were between 0.7 and 1.7ms, at contralateral recording electrodes. At ipsilateral recording electrodes, CI artifact durations are range from 0.7 to larger than 2ms.ConclusionAt contralateral recording electrodes, the artifact was shorter than the interpulse interval across subjects for 500pps, which was not always the case for 900pps.SignificanceCI artifact-free EASSRs are crucial for reliable CI fitting and neuroscience research. The CI artifact has been characterized and linear interpolation allows to remove it at contralateral recording electrodes for stimulation at 500pps.\"\r\n",
    "start = 9\r\n",
    "end = 60\r\n",
    "print(t[start-2:end+1])\r\n",
    "print(\"原始数据\",tokenizer3(t[start-1:end])[\"input_ids\"])\r\n",
    "print(tokenizer3.decode(tokenizer3(t[start:end])[\"input_ids\"]))\r\n",
    "print(len(tokenizer3(t[start:end])[\"input_ids\"]))\r\n",
    "len1 = len(tokenizer3(t[start:end])[\"input_ids\"])\r\n",
    "print(\"前置数据\",tokenizer3(t[0:start])[\"input_ids\"])\r\n",
    "print(len(tokenizer3(t[0:start])[\"input_ids\"]))\r\n",
    "len2 = len(tokenizer3(t[0:start])[\"input_ids\"])\r\n",
    "print(\"整个句子\",tokenizer3(t)[\"input_ids\"])\r\n",
    "print(len(tokenizer3(t)[\"input_ids\"]))\r\n",
    "print(\"还原\",tokenizer3.decode(tokenizer3(t)[\"input_ids\"][len2-1:len2+len1-3]))\r\n",
    "print(\"还原\",tokenizer3(t)[\"input_ids\"][len2-1:len2+len1-3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "veElectrically evoked auditory steady-state responses \n",
      "原始数据 [101, 24315, 22471, 14735, 9215, 23408, 23461, 28042, 6706, 1011, 2110, 10960, 102]\n",
      "[CLS] electrically evoked auditory steady - state responses [SEP]\n",
      "10\n",
      "前置数据 [101, 7863, 102]\n",
      "3\n",
      "整个句子 [101, 7863, 12260, 22601, 3973, 23408, 23461, 28042, 6706, 1011, 2110, 10960, 1006, 19413, 4757, 2869, 1007, 2024, 15756, 4022, 2015, 7594, 1999, 1996, 16175, 10127, 21890, 24915, 1006, 25212, 2290, 1007, 1999, 3433, 2000, 15861, 8187, 4499, 3591, 1010, 2005, 2742, 1010, 2083, 1037, 2522, 2818, 19738, 2099, 27159, 1006, 25022, 1007, 1012, 19413, 4757, 2869, 2071, 9280, 2022, 2109, 2005, 7863, 25022, 11414, 1012, 2174, 1010, 25212, 2290, 7755, 2024, 19450, 2007, 5992, 25022, 10471, 1012, 1999, 2023, 3259, 1010, 2057, 7356, 1996, 25022, 10471, 2005, 18847, 18155, 2906, 5549, 20858, 1998, 16330, 2012, 2029, 8187, 3446, 1010, 7399, 6970, 18155, 3370, 2058, 1996, 4742, 2112, 19450, 2007, 25022, 20785, 2003, 3144, 1012, 4725, 6895, 10471, 2020, 7356, 2011, 2965, 1997, 2037, 22261, 3930, 4972, 1998, 9367, 1012, 3463, 6895, 20785, 9367, 2015, 2020, 2090, 1014, 1012, 1021, 1998, 1015, 1012, 1021, 5244, 1010, 2012, 24528, 28277, 3405, 28688, 2015, 1012, 2012, 12997, 27572, 24932, 2389, 3405, 28688, 2015, 1010, 25022, 20785, 9367, 2015, 2024, 2846, 2013, 1014, 1012, 1021, 2000, 3469, 2084, 1016, 5244, 1012, 7091, 4017, 24528, 28277, 3405, 28688, 2015, 1010, 1996, 20785, 2001, 7820, 2084, 1996, 6970, 14289, 4877, 2063, 13483, 2408, 5739, 2005, 3156, 28281, 1010, 2029, 2001, 2025, 2467, 1996, 2553, 2005, 7706, 28281, 1012, 7784, 6895, 20785, 1011, 2489, 19413, 4757, 2869, 2024, 10232, 2005, 10539, 25022, 11414, 1998, 23700, 2470, 1012, 1996, 25022, 20785, 2038, 2042, 7356, 1998, 7399, 6970, 18155, 3370, 4473, 2000, 6366, 2009, 2012, 24528, 28277, 3405, 28688, 2015, 2005, 20858, 2012, 3156, 28281, 1012, 102]\n",
      "259\n",
      "还原 ##electrically evoked auditory steady -\n",
      "还原 [12260, 22601, 3973, 23408, 23461, 28042, 6706, 1011]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "print(tokenizer3.decode([19382, 8156, 1997, 15415, 3043]))\r\n",
    "print(tokenizer3.decode([19382, 8156, 1997, 15415, 4717, 2063]))\r\n",
    "tokenizer3.decode([4717, ])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "maceration of vegetable matter\n",
      "maceration of vegetable matte\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'matt'"
      ]
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "[8201, 2292, 636, 7590, 4]\r\n",
    "print(t[966:983][0])\r\n",
    "print(tokenizer.decode([24704, 5593, 13871, 2292, 636, 7590, 4]))\r\n",
    "print(tokenizer.decode([36]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "r\n",
      " manuallyremoving spicule.\n",
      " (\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "with open(r\"datasets\\semeval2017\\test\\semeval_articles_test\\S2212671612000741.txt\", 'r') as file_ann:\r\n",
    "    for line in file_ann:\r\n",
    "        kp_token = tokenizer(line[91:95])\r\n",
    "        print(kp_token)\r\n",
    "        kp_token_len = len(kp_token[\"input_ids\"])\r\n",
    "        pretoken = tokenizer(line[0:91])\r\n",
    "        pretoken_len = len(pretoken[\"input_ids\"])\r\n",
    "        keyphrase_ids = list(range(pretoken_len-1,pretoken_len-1+kp_token_len-2))\r\n",
    "        sentence_token = tokenizer(line.strip())\r\n",
    "        print(keyphrase_ids)\r\n",
    "        print([sentence_token[\"input_ids\"][keyphrase_idx] for keyphrase_idx in keyphrase_ids])\r\n",
    "        # print(token.word_ids())\r\n",
    "        print(sentence_token)\r\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [0, 530, 5330, 448, 2], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "[17, 18, 19]\n",
      "[530, 5330, 448]\n",
      "{'input_ids': [0, 250, 3645, 22432, 1421, 716, 15, 2771, 14885, 8, 47139, 19188, 49220, 29830, 9018, 154, 36, 530, 5330, 448, 43, 5448, 16, 1850, 4, 96, 42, 1421, 6, 10, 37015, 36173, 13, 3645, 12432, 154, 16, 4829, 30, 5, 20097, 9, 33900, 11305, 9658, 30, 5, 2771, 14885, 6, 215, 25, 36912, 17505, 6, 46930, 6, 5933, 8, 780, 19830, 6, 4753, 10116, 172, 42, 37015, 36173, 16, 341, 7, 12558, 10, 5163, 36173, 13, 3645, 12432, 154, 131, 1747, 6, 6168, 30759, 5, 3645, 12432, 2963, 30, 229, 5330, 448, 4, 41701, 775, 22827, 14, 84, 1421, 9980, 33334, 81, 5, 26454, 18, 467, 15, 5516, 143, 3505, 9, 3645, 12432, 2963, 6, 19, 389, 207, 746, 3645, 22432, 5849, 731, 20910, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "keyphrase = []\r\n",
    "for filename in alldata:\r\n",
    "    keyphrase_single = []\r\n",
    "    with open(os.path.join(datadir_train,filename + \".ann\"), 'r') as file_ann:\r\n",
    "        for line in file_ann:\r\n",
    "            pass\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "class semeval2017_dataset(Dataset):\r\n",
    "    def __init__(self,tokenizer):\r\n",
    "        self.datasetname = \"semeval2017\"\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.dataset = []\r\n",
    "        self.dataset_len = 0\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dataset)\r\n",
    "    def __getitem__(self,index):\r\n",
    "        return self.dataset[index]\r\n",
    "\r\n",
    "    def get_fileNames(self,root, suffix=None):\r\n",
    "        if not os.path.isabs(root):\r\n",
    "            root = os.path.join(os.getcwd(),root)\r\n",
    "        names = os.listdir(root)\r\n",
    "        result = []\r\n",
    "        if suffix:\r\n",
    "            for name in names:\r\n",
    "                if os.path.splitext(name)[1] == suffix:\r\n",
    "                    result.append(os.path.splitext(name)[0])\r\n",
    "        else:\r\n",
    "            result = names\r\n",
    "        return result\r\n",
    "    def load_data_from(self,datadir):\r\n",
    "        self.filenames = self.get_fileNames(datadir,\".txt\")\r\n",
    "        self.dataset_len = len(self.filenames)\r\n",
    "        origin_data={}\r\n",
    "        origin_data[\"text\"]=[]\r\n",
    "        origin_data[\"keyphrase\"]=[]\r\n",
    "        flict_pos_num = 0\r\n",
    "        for filename in tqdm(self.filenames):\r\n",
    "            with open(os.path.join(datadir,filename+\".txt\"), 'r',encoding='utf-8') as file_txt:\r\n",
    "                for line in file_txt:\r\n",
    "                    origin_data[\"text\"].append(line.strip())\r\n",
    "                    break\r\n",
    "            with open(os.path.join(datadir,filename+\".ann\"), 'r',encoding='utf-8') as file_ann:\r\n",
    "                keyphrase_single = []\r\n",
    "                for line in file_ann:\r\n",
    "                    line = line.strip().split(\"\\t\")\r\n",
    "                    keyphrase_single.append(line)\r\n",
    "                origin_data[\"keyphrase\"].append(keyphrase_single)\r\n",
    "        assert self.dataset_len == len(origin_data[\"text\"]) == len( origin_data[\"keyphrase\"]) , \"lengths are not same.\"\r\n",
    "        for i in tqdm(range(self.dataset_len)):\r\n",
    "            text = origin_data[\"text\"][i]\r\n",
    "            keyphrases = origin_data[\"keyphrase\"][i]\r\n",
    "            text_token = tokenizer(text)\r\n",
    "            text_len = len(text_token[\"input_ids\"])\r\n",
    "            keyphrase_pos = []\r\n",
    "            for keyphrase in keyphrases:\r\n",
    "                # 暂且只考虑T的关键词，且不考虑分类\r\n",
    "                if keyphrase[0][0] is not 'T': \r\n",
    "                    continue\r\n",
    "                tmp = keyphrase[1].split(\" \")\r\n",
    "                keyphrase_type,start,end = tmp[0], int(tmp[1]), int(tmp[2])\r\n",
    "                pretoken = self.tokenizer(text[0:start])\r\n",
    "                pretoken_len = len(pretoken[\"input_ids\"])\r\n",
    "                kp_token = self.tokenizer(text[start:end])\r\n",
    "                kp_token_len = len(kp_token[\"input_ids\"])\r\n",
    "                pos = [text_token[\"input_ids\"][keyphrase_idx] for keyphrase_idx in range(pretoken_len-1,pretoken_len-1+kp_token_len-2)]\r\n",
    "                keyphrase_pos.append(pos)\r\n",
    "\r\n",
    "            pos_label = [0] * text_len\r\n",
    "            \r\n",
    "            for pos_group in keyphrase_pos: \r\n",
    "                for pos in pos_group: \r\n",
    "                    if pos_label[pos] == 1: \r\n",
    "                        flict_pos_num += 1\r\n",
    "                    else:\r\n",
    "                        pos_label[pos]=1\r\n",
    "            text_token['labels'] = pos_label\r\n",
    "            self.dataset.append(text_token)\r\n",
    "\r\n",
    "\r\n",
    "            \r\n",
    "            \r\n",
    "                            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "dataset = semeval2017_dataset(tokenizer)\r\n",
    "print(dataset)\r\n",
    "dataset.load_data_from(datadir_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<__main__.semeval2017_dataset object at 0x000001755566DFC8>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1252.51it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18584/949253517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msemeval2017_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18584/2570162462.py\u001b[0m in \u001b[0;36mload_data_from\u001b[1;34m(self, datadir)\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mkp_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mkp_token_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkp_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeyphrase_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkeyphrase_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretoken_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpretoken_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mkp_token_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                 \u001b[0mkeyphrase_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18584/2570162462.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mkp_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mkp_token_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkp_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeyphrase_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkeyphrase_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretoken_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpretoken_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mkp_token_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                 \u001b[0mkeyphrase_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('keyphrase': conda)"
  },
  "interpreter": {
   "hash": "12a5cde4c6e74e6bab9359993eeccc978c67e84306b970b98350808efb6645f9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}